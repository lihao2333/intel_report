\pagenumbering{arabic}

\section{绪论}
%\subsection{背景介绍}
%\subsection{我们要做什么}
%\subsection{我们的最终效果}
\subsection{背景介绍}\label{section:background}

\subsubsection{人对个性化的需求}
我们的项目基于一种假设:在互联网时代，每个人都希望自己是与众不同的.
人们喜欢发朋友圈来记录自己的生活，并且从中获得满足．
互联网连接了更多的人的同时,也强化了人们的共性．
举个例子, 小王去一个景点拍照留念,并且想发到朋友圈中留念.
然而他的照片在朋友圈中显得微不足道,因为没有独特的视角,也没有独特的美景,仅仅是人不同.
人们看到自己喜欢,珍视的东西总是喜欢独自占有,这也是撞衫会令人尴尬的原因

基于深度学习的应用变得越来越广泛，其中就有一些针对图像风格变换的深度学习架构，能够实现图像内容和风格的重新组合．
对于使用者来说，只要选择一副的图片风格，经过必要的配置，就能实现风格迁移.
还是之前的例子, 小王如果喜欢著名作品scream的风格, 他可以很合成出图\ref{fig:travel}的结果.
\addfig[1]{travel.png}{fig:travel}{风格迁移示例}

我们猜测人们更喜欢这种结合了自己所喜爱元素并且独一无二的照片.

\subsubsection{风格迁移的发展}\label{section:neural-style}
基于深度学习的风格变换主要分为两个阶段,其概况如表\ref{tbl:compare-neural-style}所示．
\addtbl{compare-neuralstyle}

第一阶段, 该论文最大的贡献在于从数学的方式定义了什么是content什么是style.
网络结构以VGG-19为基础，在特定位置添加了自定义的两种loss层来分别记录content和style的loss.
该网络分为两种模式，一种是capture模式，该模式分别下前向传播content图像和style图像，对应的loss层会记录该层的输出值．
另一种是loss模式，　该模式下将一张随机图像作为输入，通过前向传播计算对应content loss 和style　loss. 
然后通过反向传播去改变输入图像．

和一般网络的反向传播不同, 该网络最终改变的不是网络权值而是输入图像,这也是最终的输出结果.
由于每一次的合成都是多次迭代,所以需要消耗很多时间.合成一副图像在cpu条件下需要大概5个多小时(具体还取决于图片尺寸,优化器和模型的选择)

第二阶段，该论文主要是拟补第一阶段速度上的不足．该网络可以看做一个图像转换层加上一个失真层，其中失真层用来统计图像转换层的输出在功能和风格上的总失真．
结合大量内容图片和一张风格图片训练，通过改变图像转换层的权值，使得失真层的输出最小, 并将最终结果存储为一个模型，　其可以认为是该风格的滤镜．
这样一来，一张新的内容图片只需要通过该模型进行以此前向传播就可以得到结果．
结果的细致程度可能比不上第一阶段，　但是由于只进行一次前向传播，耗时比较少，一般在10秒量级．而且，图片的效果可以通过训练数据集的选取来进行拟补．
其缺点是，如果要生成新的模型，训练时间比较漫长．

\subsubsection{opencl的兴起}\label{sec:opencl}
 opencl(Open Computing Language)是一个开源的标准，它的目的是为了提供一个通用的接口使得开发人员能够更容易的让设备辅助cpu进行运算．设备可以使gpu,fpga, dsp等等．它是又Apple在2008年提出，并且收到Nvidia, AMD, Intel, IBM等巨头公司合推的一个项目．在此之前加速往往采用Nvidia的cuda, 但是现在只要该设备支持opencl, 就可以用来辅助计算．
\addfig[0.8]{opencl.jpg}{fig:opencl}{opencl框架}

主办方提供给我们的基于cyclone V 的FPGA C5P就支持opencl,并且配有专门的sdk(aocl)来进行编译．
opencl框架如图\ref{fig:opencl},使用者如果想要用其加速需要做两件事
\begin{enumerate}
  \item 写目标机文件，就是opencl文件.　其定义了fpga需要做哪些事．然后使用aocl能够直接编译成aocx文件，　通过aocl提供的工具来配置fpga.
  \item 写宿主机文件，就是cpp文件. 其定义了宿主机如何与设备(fpga)进行通信.然后依靠sdk提供的库文件,头文件进行编译.
\end{enumerate}

但是opencl的代码写起来过于底层, 仅仅写一个向量加的函数,宿主机文件就需要200多行代码.用于快速生产时,一般采用一些高级的框架, 如表\ref{tbl:compare-opencl-tools}
\addtbl{compare-opencltools}
虽然同为opencl设备,但是fpga和gpu是不一样的.
opencl设备一般工作流程如下:
\begin{enumerate}
  \item 程序启动
  \item 程序初始化gpu设备
  \item	程序加载cl文件
  \item	程序把cl文件编译为中间文件,然后传递给gpu,获得一个句柄函数
  \item 程序通过句柄函数把数据发送给gpu,并且等待回传结果
\end{enumerate}

对于一般的gpu设备,步骤1到步骤3需要大概几秒钟,第四步需要几秒钟,第五步时间取决于数据类型和数据量
对于fpga, 第4步的时间将会非常的漫长,在小时的数量级,所以fpga不能采用实时编译的方式,必须提前把cl文件编译号烧录到fpga中.

然而,表\ref{tbl:compare-opencl-tools}所示的前三种都主要针对gpu类型的opencl设备,第四种虽然针对fpga, 但是我们尝试编译c5p的时候没有编译成功.

\subsubsection{Movidius神经网络加速棒的兴起}\label{sec:movidius}
Movidius神经计算棒是世界上第一款采用USB格式的AI加速器,这个计算棒能够编译并加速边缘神经网络。而且，这款产品并不需要连接到云端，可以直接在本地实现处理。它的工作原理如图\ref{fig:movidius}所示.使用者将训练好的caffe模型或者tensorflow模型用sdk编译,烧录到movidius中. 然后就可以通过sdk提供的api进行加速前向传播.
\addfig[0.7]{movidius.png}{fig:movidius}{movidius工作流程}
该产品目前只支持tensorflow和caffe两种深度学习框架,并且提供了一个docker,里面安装了tensorflow,caffe和对应sdk(NC).
\subsection{我们要做什么}
如\ref{section:neural-style}节所示，两种风格迁移的方式各有优缺点,neural-style能够对任意的内容图片和风格图片进行融合, 并且非常细微,能够输出每一步骤的结果,但是耗时太久,合成一张图片需要的时间在几个小时的数量量级. 而fast-neural-style能够在已经训练好模型的前提下,能够快速的将模型对应的风格叠加到内容图片上, 耗时在十秒左右,缺点是对于新的模型需要单独用大量数据进行长时间的训练.

而另一方面,主办方提供给我们的fpga和movidius神经网络加速棒都有加速的功能,前者能够加速训练,后者能够加速前向传播的过程.所以我们就想用加速装置来拟补两种方法的不足.

neural-style能够输出每一步的结果,那么我们就将每一步的结果合成为一个视频, 给用户展示一个从现实到梦幻演变的过程.另外我们采用支持opencl的深度学习的架构进行训练,利用fpga加速训练.不过就算加速后可能也不能到达实时的效果.但是能够大幅度等待用户的等待时间.

fast-neural-style能够快速输出一张质量还不错的结果, 那么我们就预置一些精选的模型,
用户选择一个中意的模型后能够快速得到结果, 经过fast-neural-style已经能够在十几秒左右完成一次前向传播,
但是这对于人的体验影响依然很大,我们采用movidius进行加速前向传播,尽可能的减少时间, 给人以实时的感觉

我们的架构设计如图\ref{fig:top}我们在up2的主机上搭建服务器,用户能够通过网页访问服务器. 
有两种模式供用户选择,分别是生成视频和生成图像, 本质上是选择用neural-style还是fast-neural-style。
\addfig[0.9]{top.jpg}{fig:top}{总体架构图}

如果选择生成视频， 那么用户上传自己的内容图片和风格图片，
提交后后台就会开始利用fpga进行加速训练,当然提前要把通过opencl文件编译的镜像通过aocl工具烧录到fpga中。
在训练的过程中会产生一些列图片，训练完成后会将这些图片合并为一个视频.
生成的结果保存在放在和用户匹配的路径下, 过一段时间用户就可以通过网页查看输出结果，
可以看到一个从原来图片到目标图片的一个视频，并且能够自由下载相关的所有图片和视频。

如果选择生成图片，那么用户上传自己的内容图片并且在提供的风格类型中进行选择.
提交后后台就会调用对应的模型，利用movidius进行加速前向传播.当然提前要把模型通过NC进行编译并且烧录进去.
输出结果会保存在和用户对应的路径下，用户可以通过网页查看输出结果并且能够自由下载相关图片

\subsection{我们的最终效果}
最终,我们实现了用户登录网页后,通过上传内容图片和风格图片就能够得到一个视频,通过上传内容图片和选择风格模型快速得到一张图片.
具体流程可以参考\ref{sec:ui}节所示
加速器方面我们实现了用movidius来加速产生图片的前线传播过程.
在使用fpga加速产生视频部分的时候,还参与一些问题,希望能够在评审前攻克.
